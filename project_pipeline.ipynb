{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7664943a-2a31-424b-aef3-36b79d5362a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T11:42:12.941479Z",
     "start_time": "2024-11-11T11:42:03.361066Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad705af5-21ab-4778-b30e-04a1d53db59d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T11:42:34.709137Z",
     "start_time": "2024-11-11T11:42:18.400387Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "# Manipulation de données\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Traitement du signal\n",
    "from scipy import signal\n",
    "import mne\n",
    "\n",
    "# Machine Learning et Deep Learning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import torch\n",
    "\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#import plotly.express as px\n",
    "\n",
    "# Gestion de Notebooks\n",
    "#import papermill as pm\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Utilitaires\n",
    "import joblib\n",
    "import yaml\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Importation code local\n",
    "sys.path.append('preprocessing')\n",
    "import preprocess\n",
    "import dataset\n",
    "#import torchcam\n",
    "#import models.GGN.ggn_model as GGN\n",
    "#import models.GGN.train as train\n",
    "#importlib.reload(preprocess)\n",
    "#importlib.reload(GGN)\n",
    "#importlib.reload(train)\n",
    "#importlib.reload(dataset)\n",
    "\n",
    "print('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d0c9a8",
   "metadata": {},
   "source": [
    "## Chargement de la configuration du projet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75051ae0-dd5a-4fa5-bfc1-963b87f12360",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T11:42:37.629398Z",
     "start_time": "2024-11-11T11:42:37.581228Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chargement de la configuration YAML\n",
    "with open(\"config.yml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7b6404c-ca05-4d1d-bdc7-0d5a3dad999a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Répertoire courant : /project/166600089/mathis-add-noises\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(f\"Répertoire courant : {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fb7966",
   "metadata": {},
   "source": [
    "## Noisy adjustements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2175e3f3-a415-4ddd-b5ae-6b7826e73318",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T11:42:58.823832Z",
     "start_time": "2024-11-11T11:42:40.477590Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "\n",
    "def add_noise_to_data(data, noise_level=0.1):\n",
    "    \"\"\"\n",
    "    Ajoute un bruit gaussien aux données EEG.\n",
    "    Arguments :\n",
    "    - data : ndarray, les données EEG (n_epochs, n_channels, n_times).\n",
    "    - noise_level : float, intensité du bruit (fraction de l'amplitude max des données).\n",
    "    Retourne :\n",
    "    - data_noisy : ndarray, les données avec du bruit ajouté.\n",
    "    \"\"\"\n",
    "    noise = np.random.normal(0, noise_level * np.max(data), size=data.shape)\n",
    "    return data + noise\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87b5aced-834f-4cae-a7b9-dc948079391d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_and_save_noisy_data(base_dir, output_dir, noise_level=0.1):\n",
    "    \"\"\"\n",
    "    Parcourt les fichiers .fif, ajoute du bruit et sauvegarde les fichiers bruités dans le sous-dossier parent.\n",
    "    Arguments :\n",
    "    - base_dir : str, répertoire contenant les fichiers originaux.\n",
    "    - output_dir : str, répertoire où sauvegarder les fichiers bruités.\n",
    "    - noise_level : float, intensité du bruit.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(base_dir):\n",
    "        raise FileNotFoundError(f\"Le chemin spécifié n'existe pas : {base_dir}\")\n",
    "    \n",
    "    print(f\"Chemin de base : {base_dir}\")\n",
    "    print(f\"Contenu du répertoire : {os.listdir(base_dir)}\")\n",
    "    \n",
    "    fif_files = [\n",
    "        os.path.join(root, file)\n",
    "        for root, dirs, files in os.walk(base_dir)\n",
    "        for file in files if file.endswith(\"-epo.fif\")\n",
    "    ]\n",
    "    \n",
    "    if not fif_files:\n",
    "        print(\"Aucun fichier .fif trouvé dans le répertoire spécifié.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Nombre de fichiers .fif trouvés : {len(fif_files)}\")\n",
    "    \n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        print(f\"Exploration du répertoire : {root}\")\n",
    "        for file in files:\n",
    "            if file.endswith(\"-epo.fif\"):\n",
    "                print(f\"Traitement du fichier : {file}\")\n",
    "                file_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(root, base_dir)\n",
    "                noisy_path = os.path.join(output_dir, relative_path)  # Dossier parent sans \"noisy\"\n",
    "                os.makedirs(noisy_path, exist_ok=True)\n",
    "\n",
    "                try:\n",
    "                    # Charger les données\n",
    "                    epochs = mne.read_epochs(file_path, preload=True)\n",
    "                    data = epochs.get_data()\n",
    "                    print(f\"Shape des données : {data.shape}\")\n",
    "                    \n",
    "                    # Ajouter du bruit\n",
    "                    data_noisy = add_noise_to_data(data, noise_level)\n",
    "                    epochs._data = data_noisy\n",
    "                    \n",
    "                    # Sauvegarder les données bruitées dans le dossier parent\n",
    "                    noisy_file_path = os.path.join(noisy_path, file)\n",
    "                    epochs.save(noisy_file_path, overwrite=True)\n",
    "                    print(f\"Fichier bruité sauvegardé : {noisy_file_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur lors du traitement du fichier {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9427a6-fb55-4f51-9bef-92dc2005722d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "base_dir = os.path.join(os.getcwd(), \"data/processed\")  # Répertoire de base contenant les fichiers originaux\n",
    "output_dir = os.path.join(os.getcwd(), \"data/noisy\") # Répertoire de sortie pour les fichiers bruités\n",
    "\n",
    "print(base_dir)\n",
    "print(output_dir)\n",
    "noise_level = 0.3  # Intensité du bruit\n",
    "\n",
    "process_and_save_noisy_data(base_dir, output_dir, noise_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76cabb0c-0f4b-47c8-a9ea-2610022378ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subject: sub-022\n",
      "Processing task: audiopassive\n",
      "Saved splits for sub-022 task audiopassive in data/split_noisy\n",
      "Processing task: audioactive\n",
      "Saved splits for sub-022 task audioactive in data/split_noisy\n",
      "Processing task: thermalpassive\n",
      "Saved splits for sub-022 task thermalpassive in data/split_noisy\n",
      "Processing task: thermalactive\n",
      "Saved splits for sub-022 task thermalactive in data/split_noisy\n",
      "Processing subject: sub-028\n",
      "Processing task: thermalpassive\n",
      "Saved splits for sub-028 task thermalpassive in data/split_noisy\n",
      "Processing task: audiopassive\n",
      "Saved splits for sub-028 task audiopassive in data/split_noisy\n",
      "Processing task: audioactive\n",
      "Saved splits for sub-028 task audioactive in data/split_noisy\n",
      "Processing task: thermalactive\n",
      "Saved splits for sub-028 task thermalactive in data/split_noisy\n",
      "Processing subject: sub-010\n",
      "Processing task: audiopassive\n",
      "Saved splits for sub-010 task audiopassive in data/split_noisy\n",
      "Processing task: audioactive\n",
      "Saved splits for sub-010 task audioactive in data/split_noisy\n",
      "Processing task: thermalactive\n",
      "Saved splits for sub-010 task thermalactive in data/split_noisy\n",
      "Processing task: thermalpassive\n",
      "Saved splits for sub-010 task thermalpassive in data/split_noisy\n",
      "Processing subject: sub-031\n",
      "Processing task: thermalpassive\n",
      "Saved splits for sub-031 task thermalpassive in data/split_noisy\n",
      "Processing task: thermalactive\n",
      "Saved splits for sub-031 task thermalactive in data/split_noisy\n",
      "Processing task: audioactive\n",
      "Saved splits for sub-031 task audioactive in data/split_noisy\n",
      "Processing task: audiopassive\n",
      "Saved splits for sub-031 task audiopassive in data/split_noisy\n",
      "Processing subject: sub-009\n",
      "Processing task: thermalactive\n",
      "Saved splits for sub-009 task thermalactive in data/split_noisy\n",
      "Processing task: audiopassive\n",
      "Saved splits for sub-009 task audiopassive in data/split_noisy\n",
      "Processing task: thermalpassive\n",
      "Saved splits for sub-009 task thermalpassive in data/split_noisy\n",
      "Processing task: audioactive\n",
      "Saved splits for sub-009 task audioactive in data/split_noisy\n",
      "Processing subject: sub-003\n",
      "Processing task: audioactive\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 1] Operation not permitted: 'data/split_noisy/train/sub-003/audioactive/16-epo.fif'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m path_noisy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/noisy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m path_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/split_noisy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtrain_test_split_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_noisy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project/166600089/mathis-add-noises/preprocessing/preprocess.py:184\u001b[0m, in \u001b[0;36mtrain_test_split_files\u001b[0;34m(preprocessed_data_path, output_path, test_size, random_state)\u001b[0m\n\u001b[1;32m    182\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(split_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[0;32m--> 184\u001b[0m         \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved splits for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubject_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.11.5/lib/python3.11/shutil.py:420\u001b[0m, in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    418\u001b[0m     dst \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dst, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(src))\n\u001b[1;32m    419\u001b[0m copyfile(src, dst, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n\u001b[0;32m--> 420\u001b[0m \u001b[43mcopymode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.11.5/lib/python3.11/shutil.py:309\u001b[0m, in \u001b[0;36mcopymode\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    306\u001b[0m     stat_func, chmod_func \u001b[38;5;241m=\u001b[39m _stat, os\u001b[38;5;241m.\u001b[39mchmod\n\u001b[1;32m    308\u001b[0m st \u001b[38;5;241m=\u001b[39m stat_func(src)\n\u001b[0;32m--> 309\u001b[0m chmod_func(dst, stat\u001b[38;5;241m.\u001b[39mS_IMODE(st\u001b[38;5;241m.\u001b[39mst_mode))\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 1] Operation not permitted: 'data/split_noisy/train/sub-003/audioactive/16-epo.fif'"
     ]
    }
   ],
   "source": [
    "# Appel de la fonction split\n",
    "\n",
    "from preprocessing.preprocess import train_test_split_files\n",
    "path_noisy = \"data/noisy\"\n",
    "path_split = \"data/split_noisy\"\n",
    "\n",
    "train_test_split_files(path_noisy, path_split, 0.2, 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c37280",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf7aa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_model = config['project_config']['running_model']\n",
    "\n",
    "subjects_id = config['data']['subjects']\n",
    "\n",
    "bids_root = config['data']['path']\n",
    "\n",
    "\n",
    "test_losses = []\n",
    "accuracies = []\n",
    "recalls = []\n",
    "precisions = []\n",
    "f1_scores = []\n",
    "auc_rocs = []\n",
    "\n",
    "if not subjects_id:\n",
    "        subjects_id = [\n",
    "            d for d in os.listdir(bids_root)\n",
    "            if os.path.isdir(os.path.join(bids_root, d)) and d.startswith(\"sub-\")\n",
    "        ]\n",
    "        print(f\"Aucun ID de sujet spécifié. Tous les sujets détectés : {subjects_id}\")\n",
    "\n",
    "for subject in subjects_id:\n",
    "\n",
    "    if running_model == \"GGN\":\n",
    "        train_loader, val_loader, test_loader = dataset.create_dataloader([subject], config)\n",
    "\n",
    "        model = GGN.GGN(**config['models']['GGN']['parameters'], device=device)\n",
    "        \n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        num_epochs = 50\n",
    "\n",
    "        # Train and validate the model\n",
    "        train.train(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\n",
    "\n",
    "        # Test the model\n",
    "        avg_test_loss, accuracy, recall, precision, f1, auc_roc = train.test(model, test_loader, criterion, device)\n",
    "        test_losses.append(avg_test_loss)\n",
    "        accuracies.append(accuracy)\n",
    "        recalls.append(recall)\n",
    "        precisions.append(precision)\n",
    "        f1_scores.append(f1)\n",
    "        auc_rocs.append(auc_roc)\n",
    "        \n",
    "        model.explain_temporal_cnn(test_loader, device)\n",
    "        \n",
    "\n",
    "    elif running_model == \"SVM\":\n",
    "        train_loader, val_loader, test_loader = dataset.create_dataloader([subject], config)\n",
    "\n",
    "        # Convert data loaders to numpy arrays\n",
    "        X_train, y_train = preprocess.dataloader_without_topology_to_numpy(train_loader)\n",
    "        X_val, y_val = preprocess.dataloader_without_topology_to_numpy(val_loader)\n",
    "        X_test, y_test = preprocess.dataloader_without_topology_to_numpy(test_loader)\n",
    "        \n",
    "        print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "        print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "        print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "        # Reshape data\n",
    "        X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "        X_val = X_val.reshape(X_val.shape[0], -1)\n",
    "        X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "        # Train the SVM\n",
    "        svm_params = config['models']['SVM']['parameters']\n",
    "        svm = SVC(**svm_params)\n",
    "        svm.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        y_val_pred = svm.predict(X_val)\n",
    "        val_acc = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "        # Test the model\n",
    "        y_test_pred = svm.predict(X_test)\n",
    "        test_acc = accuracy_score(y_test, y_test_pred)\n",
    "        accuracies.append(test_acc)\n",
    "\n",
    "# Calculate mean test accuracy across all subjects\n",
    "mean_test_loss = np.mean(test_losses)\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "mean_recall = np.mean(recalls)\n",
    "mean_precision = np.mean(precisions)\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "mean_auc_roc = np.mean(auc_rocs)\n",
    "\n",
    "print(f\"Mean Test Loss across all subjects: {mean_test_loss:.4f}\")\n",
    "print(f\"Mean Test Accuracy across all subjects: {mean_accuracy:.2f}%\")\n",
    "print(f\"Mean Recall (Sensitivity) across all subjects: {mean_recall:.2f}\")\n",
    "print(f\"Mean Precision across all subjects: {mean_precision:.2f}\")\n",
    "print(f\"Mean F1 Score across all subjects: {mean_f1:.2f}\")\n",
    "print(f\"Mean AUC-ROC across all subjects: {mean_auc_roc:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
