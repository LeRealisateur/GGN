{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7664943a-2a31-424b-aef3-36b79d5362a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T11:42:12.941479Z",
     "start_time": "2024-11-11T11:42:03.361066Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install mne_connectivity --no-deps\n",
    "!pip install xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad705af5-21ab-4778-b30e-04a1d53db59d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T02:43:46.317563Z",
     "start_time": "2024-12-19T02:43:10.103935Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Manipulation de données\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Traitement du signal\n",
    "from scipy import signal\n",
    "import mne\n",
    "\n",
    "# Machine Learning et Deep Learning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import torch\n",
    "\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#import plotly.express as px\n",
    "\n",
    "# Gestion de Notebooks\n",
    "#import papermill as pm\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Utilitaires\n",
    "import joblib\n",
    "import yaml\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Importation code local\n",
    "sys.path.append('preprocessing')\n",
    "import preprocess\n",
    "import dataset\n",
    "import noise\n",
    "import torchcam\n",
    "import models.GGN.ggn_model as GGN\n",
    "import models.GGN.train as train\n",
    "import models.EEGNet.EEGNet as EEGNet\n",
    "import models.EEGDeformer.EEGDeformer as EEGDeformer\n",
    "sys.path.append('evaluation')\n",
    "import within_session\n",
    "import noise\n",
    "import saliency_map\n",
    "from preprocessing.preprocess import train_test_split_files\n",
    "importlib.reload(preprocess)\n",
    "importlib.reload(GGN)\n",
    "importlib.reload(train)\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(within_session)\n",
    "importlib.reload(noise)\n",
    "importlib.reload(saliency_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d0c9a8",
   "metadata": {},
   "source": [
    "## Chargement de la configuration du projet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75051ae0-dd5a-4fa5-bfc1-963b87f12360",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T02:43:48.556050Z",
     "start_time": "2024-12-19T02:43:48.530215Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chargement de la configuration YAML\n",
    "with open(\"config.yml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    \n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Appliquer le seed depuis la config\n",
    "set_seed(config['project_config']['seed'])\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fb7966",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2175e3f3-a415-4ddd-b5ae-6b7826e73318",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T02:45:13.279312Z",
     "start_time": "2024-12-19T02:43:53.953562Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_dir = os.path.join(os.getcwd(), \"data/processed\")\n",
    "output_dir = os.path.join(os.getcwd(), \"data/noisy\")\n",
    "path_noisy = \"data/noisy\"\n",
    "path_split = \"data/split_noisy\"\n",
    "\n",
    "subjects_id = config['data']['subjects']\n",
    "\n",
    "preprocess.preprocess_data(config)\n",
    "noise.process_and_save_noisy_data(base_dir, output_dir, subjects_id, 0.01, 0.0005)\n",
    "preprocess.train_test_split_files(subjects_id, path_noisy, path_split, 0.2, 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c37280",
   "metadata": {},
   "source": [
    "## Model Training (Without noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf7aa33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T02:53:55.270183Z",
     "start_time": "2024-12-19T02:52:56.256998Z"
    }
   },
   "outputs": [],
   "source": [
    "running_model = config['project_config']['running_model']\n",
    "\n",
    "subjects_id = config['data']['subjects']\n",
    "\n",
    "bids_root = config['data']['path']\n",
    "\n",
    "result_save_path = config['output']['results_save_path']\n",
    "\n",
    "test_losses = []\n",
    "accuracies = []\n",
    "recalls = []\n",
    "precisions = []\n",
    "f1_scores = []\n",
    "auc_rocs = []\n",
    "\n",
    "if not subjects_id:\n",
    "        subjects_id = [\n",
    "            d for d in os.listdir(bids_root)\n",
    "            if os.path.isdir(os.path.join(bids_root, d)) and d.startswith(\"sub-\")\n",
    "        ]\n",
    "        print(f\"Aucun ID de sujet spécifié. Tous les sujets détectés : {subjects_id}\")\n",
    "\n",
    "for subject in subjects_id:\n",
    "\n",
    "    if running_model == \"GGN\":\n",
    "        raw_data = mne.io.read_raw_brainvision(\n",
    "            os.path.join(bids_root, subject, \"eeg\", f\"{subject}_task-{config['data']['tasks'][0]}_eeg.vhdr\"),\n",
    "            eog=[\"VEOG\", \"HEOG\"],\n",
    "            misc=[\"rating\", \"temp\", \"stim\"],\n",
    "            preload=True,\n",
    "        )\n",
    "        if 'Iz' in raw_data.ch_names:\n",
    "            raw_data.drop_channels([\"Iz\"])\n",
    "\n",
    "        montage = mne.channels.make_standard_montage('standard_1020')\n",
    "        raw_data.set_montage(montage)\n",
    "        coords = np.array([\n",
    "            ch['loc'][:3] for ch in raw_data.info['chs']\n",
    "            if ch['kind'] == mne.io.constants.FIFF.FIFFV_EEG_CH and np.any(ch['loc'][:3])\n",
    "        ])\n",
    "        info = raw_data.info\n",
    "\n",
    "        train_loader, val_loader, test_loader = dataset.create_dataloader([subject], config)\n",
    "\n",
    "        model = GGN.GGN(**config['models']['GGN']['parameters'], subject_id=subject, coords=coords, info=info,\n",
    "                        save_path=result_save_path)\n",
    "\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        num_epochs = 50\n",
    "\n",
    "        # Train and validate the model\n",
    "        train.train(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\n",
    "\n",
    "        # Test the model\n",
    "        avg_test_loss, accuracy, recall, precision, f1, auc_roc = train.test(model, test_loader, criterion, device)\n",
    "        test_losses.append(avg_test_loss)\n",
    "        accuracies.append(accuracy)\n",
    "        recalls.append(recall)\n",
    "        precisions.append(precision)\n",
    "        f1_scores.append(f1)\n",
    "        auc_rocs.append(auc_roc)\n",
    "        \n",
    "        model.explain_temporal_cnn(test_loader, device)\n",
    "        \n",
    "\n",
    "    elif running_model == \"SVM\":\n",
    "        train_loader, val_loader, test_loader = dataset.create_dataloader([subject], config)\n",
    "\n",
    "        # Convert data loaders to numpy arrays\n",
    "        X_train, y_train = preprocess.dataloader_without_topology_to_numpy(train_loader)\n",
    "        X_val, y_val = preprocess.dataloader_without_topology_to_numpy(val_loader)\n",
    "        X_test, y_test = preprocess.dataloader_without_topology_to_numpy(test_loader)\n",
    "        \n",
    "        print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "        print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "        print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "        # Reshape data\n",
    "        X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "        X_val = X_val.reshape(X_val.shape[0], -1)\n",
    "        X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "        # Train the SVM\n",
    "        svm_params = config['models']['SVM']['parameters']\n",
    "        svm = SVC(**svm_params)\n",
    "        svm.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        y_val_pred = svm.predict(X_val)\n",
    "        val_acc = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "        # Test the model\n",
    "        y_test_pred = svm.predict(X_test)\n",
    "        test_acc = accuracy_score(y_test, y_test_pred)\n",
    "        accuracies.append(test_acc)\n",
    "\n",
    "    elif running_model == \"EEGNet\":\n",
    "        model = EEGNet.EEGNet(nb_classes = 2).to(device)\n",
    "\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "        within_session.within_session_evaluation(config, model, criterion, optimizer, device)\n",
    "         \n",
    "    elif running_model == \"EEGDeformer\":\n",
    "        model = EEGDeformer.Deformer(num_chan=64, num_time=65, temporal_kernel=11, num_kernel=64,\n",
    "                num_classes=2, depth=4, heads=8,\n",
    "                mlp_dim=8, dim_head=8, dropout=0.).to(device)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "        within_session.within_session_evaluation(config, model, criterion, optimizer, device)\n",
    "\n",
    "# Calculate mean test accuracy across all subjects\n",
    "mean_test_loss = np.mean(test_losses)\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "mean_recall = np.mean(recalls)\n",
    "mean_precision = np.mean(precisions)\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "mean_auc_roc = np.mean(auc_rocs)\n",
    "\n",
    "print(f\"Mean Test Loss across all subjects: {mean_test_loss:.4f}\")\n",
    "print(f\"Mean Test Accuracy across all subjects: {mean_accuracy:.2f}%\")\n",
    "print(f\"Mean Recall (Sensitivity) across all subjects: {mean_recall:.2f}\")\n",
    "print(f\"Mean Precision across all subjects: {mean_precision:.2f}\")\n",
    "print(f\"Mean F1 Score across all subjects: {mean_f1:.2f}\")\n",
    "print(f\"Mean AUC-ROC across all subjects: {mean_auc_roc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bf78cb-73d2-43dc-814d-990c368e8163",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Training (With noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329112c0-8ba2-4c70-94b5-8c2f066f3473",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_model = config['project_config']['running_model']\n",
    "\n",
    "subjects_id = config['data']['subjects']\n",
    "\n",
    "bids_root = config['data']['path']\n",
    "\n",
    "config['output']['split_data_save_path'] = path_split # Change to path with noisy data\n",
    "\n",
    "result_save_path = config['output']['results_save_path']\n",
    "\n",
    "test_losses = []\n",
    "accuracies = []\n",
    "recalls = []\n",
    "precisions = []\n",
    "f1_scores = []\n",
    "auc_rocs = []\n",
    "\n",
    "if not subjects_id:\n",
    "        subjects_id = [\n",
    "            d for d in os.listdir(bids_root)\n",
    "            if os.path.isdir(os.path.join(bids_root, d)) and d.startswith(\"sub-\")\n",
    "        ]\n",
    "        print(f\"Aucun ID de sujet spécifié. Tous les sujets détectés : {subjects_id}\")\n",
    "\n",
    "for subject in subjects_id:\n",
    "\n",
    "    if running_model == \"GGN\":\n",
    "        raw_data = mne.io.read_raw_brainvision(\n",
    "            os.path.join(bids_root, subject, \"eeg\", f\"{subject}_task-{config['data']['tasks'][0]}_eeg.vhdr\"),\n",
    "            eog=[\"VEOG\", \"HEOG\"],\n",
    "            misc=[\"rating\", \"temp\", \"stim\"],\n",
    "            preload=True,\n",
    "        )\n",
    "        if 'Iz' in raw_data.ch_names:\n",
    "            raw_data.drop_channels([\"Iz\"])\n",
    "\n",
    "        montage = mne.channels.make_standard_montage('standard_1020')\n",
    "        raw_data.set_montage(montage)\n",
    "        coords = np.array([\n",
    "            ch['loc'][:3] for ch in raw_data.info['chs']\n",
    "            if ch['kind'] == mne.io.constants.FIFF.FIFFV_EEG_CH and np.any(ch['loc'][:3])\n",
    "        ])\n",
    "        info = raw_data.info\n",
    "\n",
    "        train_loader, val_loader, test_loader = dataset.create_dataloader([subject], config)\n",
    "\n",
    "        model = GGN.GGN(**config['models']['GGN']['parameters'], subject_id=subject, coords=coords, info=info,\n",
    "                        save_path=result_save_path)\n",
    "\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        num_epochs = 50\n",
    "\n",
    "        # Train and validate the model\n",
    "        train.train(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\n",
    "\n",
    "        # Test the model\n",
    "        avg_test_loss, accuracy, recall, precision, f1, auc_roc = train.test(model, test_loader, criterion, device)\n",
    "        test_losses.append(avg_test_loss)\n",
    "        accuracies.append(accuracy)\n",
    "        recalls.append(recall)\n",
    "        precisions.append(precision)\n",
    "        f1_scores.append(f1)\n",
    "        auc_rocs.append(auc_roc)\n",
    "        \n",
    "        model.explain_temporal_cnn(test_loader, device)\n",
    "        \n",
    "\n",
    "    elif running_model == \"SVM\":\n",
    "        train_loader, val_loader, test_loader = dataset.create_dataloader([subject], config)\n",
    "\n",
    "        # Convert data loaders to numpy arrays\n",
    "        X_train, y_train = preprocess.dataloader_without_topology_to_numpy(train_loader)\n",
    "        X_val, y_val = preprocess.dataloader_without_topology_to_numpy(val_loader)\n",
    "        X_test, y_test = preprocess.dataloader_without_topology_to_numpy(test_loader)\n",
    "        \n",
    "        print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "        print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "        print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "        # Reshape data\n",
    "        X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "        X_val = X_val.reshape(X_val.shape[0], -1)\n",
    "        X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "        # Train the SVM\n",
    "        svm_params = config['models']['SVM']['parameters']\n",
    "        svm = SVC(**svm_params)\n",
    "        svm.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        y_val_pred = svm.predict(X_val)\n",
    "        val_acc = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "        # Test the model\n",
    "        y_test_pred = svm.predict(X_test)\n",
    "        test_acc = accuracy_score(y_test, y_test_pred)\n",
    "        accuracies.append(test_acc)\n",
    "\n",
    "    elif running_model == \"EEGNet\":\n",
    "        model = EEGNet.EEGNet(nb_classes = 2).to(device)\n",
    "\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "        within_session.within_session_evaluation(config, model, criterion, optimizer, device)\n",
    "         \n",
    "    elif running_model == \"EEGDeformer\":\n",
    "        model = EEGDeformer.Deformer(num_chan=64, num_time=65, temporal_kernel=11, num_kernel=64,\n",
    "                num_classes=2, depth=4, heads=8,\n",
    "                mlp_dim=8, dim_head=8, dropout=0.).to(device)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "        within_session.within_session_evaluation(config, model, criterion, optimizer, device)\n",
    "\n",
    "# Calculate mean test accuracy across all subjects\n",
    "mean_test_loss = np.mean(test_losses)\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "mean_recall = np.mean(recalls)\n",
    "mean_precision = np.mean(precisions)\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "mean_auc_roc = np.mean(auc_rocs)\n",
    "\n",
    "print(f\"Mean Test Loss across all subjects: {mean_test_loss:.4f}\")\n",
    "print(f\"Mean Test Accuracy across all subjects: {mean_accuracy:.2f}%\")\n",
    "print(f\"Mean Recall (Sensitivity) across all subjects: {mean_recall:.2f}\")\n",
    "print(f\"Mean Precision across all subjects: {mean_precision:.2f}\")\n",
    "print(f\"Mean F1 Score across all subjects: {mean_f1:.2f}\")\n",
    "print(f\"Mean AUC-ROC across all subjects: {mean_auc_roc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e584f678-7f8f-4193-a151-143474fb40e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "base_dir = os.path.join(os.getcwd(), \"data/processed\")  # Répertoire contenant les fichiers d'origine\n",
    "output_dir = os.path.join(os.getcwd(), \"data/noisy\")   # Répertoire de sortie pour les fichiers bruités\n",
    "split_dir = os.path.join(os.getcwd(), \"data/split_noisy\")  # Répertoire pour les fichiers bruités split\n",
    "\n",
    "print(\"Répertoire des données traitées :\", base_dir)\n",
    "print(\"Répertoire des données bruitées :\", output_dir)\n",
    "print(\"Répertoire des données split :\", split_dir)\n",
    "\n",
    "# Visualisation d'un epoch avant et après le bruit\n",
    "epoch_file_original = os.path.join(base_dir, \"sub-003/audioactive/2-epo.fif\")  # Ajuster si nécessaire\n",
    "epoch_file_noisy = os.path.join(split_dir, \"train/sub-003/audioactive/2-epo.fif\")  # Ajuster selon l'organisation\n",
    "\n",
    "def plot_epoch_comparison(original_file, noisy_file, epoch_idx=0):\n",
    "    \"\"\"\n",
    "    Compare un epoch EEG avant et après l'ajout de bruit.\n",
    "\n",
    "    Arguments :\n",
    "    - original_file : str, chemin vers le fichier d'epochs original.\n",
    "    - noisy_file : str, chemin vers le fichier d'epochs bruité.\n",
    "    - epoch_idx : int, index de l'epoch à afficher.\n",
    "    \"\"\"\n",
    "    # Charger les epochs\n",
    "    epochs_original = mne.read_epochs(original_file, preload=True)\n",
    "    epochs_noisy = mne.read_epochs(noisy_file, preload=True)\n",
    "    \n",
    "    data_original = epochs_original.get_data()[epoch_idx]\n",
    "    data_noisy = epochs_noisy.get_data()[epoch_idx]\n",
    "    \n",
    "    ch_names = epochs_original.ch_names\n",
    "    time_points = epochs_original.times\n",
    "\n",
    "    # Affichage\n",
    "    fig, axes = plt.subplots(len(ch_names), 2, figsize=(10, len(ch_names) * 2))\n",
    "    fig.suptitle(f\"Comparison of EEG Epoch Before and After Noise (Epoch {epoch_idx})\", fontsize=14)\n",
    "    \n",
    "    for i, ch_name in enumerate(ch_names):\n",
    "        # Original signal\n",
    "        axes[i, 0].plot(time_points, data_original[i], linewidth=0.8)\n",
    "        axes[i, 0].set_title(f\"{ch_name} - Original\")\n",
    "        axes[i, 0].set_ylabel(\"Amplitude (uV)\")\n",
    "        \n",
    "        # Noisy signal\n",
    "        axes[i, 1].plot(time_points, data_noisy[i], linewidth=0.8, color='r')\n",
    "        axes[i, 1].set_title(f\"{ch_name} - Noisy\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Labels\n",
    "    for ax in axes[-1]:\n",
    "        ax.set_xlabel(\"Time (s)\")\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "# Appel de la fonction pour comparaison\n",
    "plot_epoch_comparison(epoch_file_original, epoch_file_noisy, epoch_idx=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c815b60d-4c07-4304-b964-c3469d50d872",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
