{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7664943a-2a31-424b-aef3-36b79d5362a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T11:42:12.941479Z",
     "start_time": "2024-11-11T11:42:03.361066Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad705af5-21ab-4778-b30e-04a1d53db59d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T11:42:34.709137Z",
     "start_time": "2024-11-11T11:42:18.400387Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "# Manipulation de données\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Traitement du signal\n",
    "from scipy import signal\n",
    "import mne\n",
    "\n",
    "# Machine Learning et Deep Learning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import torch\n",
    "\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#import plotly.express as px\n",
    "\n",
    "# Gestion de Notebooks\n",
    "#import papermill as pm\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Utilitaires\n",
    "import joblib\n",
    "import yaml\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Importation code local\n",
    "sys.path.append('preprocessing')\n",
    "import preprocess\n",
    "import dataset\n",
    "#import torchcam\n",
    "#import models.GGN.ggn_model as GGN\n",
    "#import models.GGN.train as train\n",
    "#importlib.reload(preprocess)\n",
    "#importlib.reload(GGN)\n",
    "#importlib.reload(train)\n",
    "#importlib.reload(dataset)\n",
    "\n",
    "print('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d0c9a8",
   "metadata": {},
   "source": [
    "## Chargement de la configuration du projet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75051ae0-dd5a-4fa5-bfc1-963b87f12360",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T11:42:37.629398Z",
     "start_time": "2024-11-11T11:42:37.581228Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chargement de la configuration YAML\n",
    "with open(\"config.yml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7b6404c-ca05-4d1d-bdc7-0d5a3dad999a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Répertoire courant : /project/166600089/mathis-add-noises\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(f\"Répertoire courant : {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fb7966",
   "metadata": {},
   "source": [
    "## Noisy adjustements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2175e3f3-a415-4ddd-b5ae-6b7826e73318",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T11:42:58.823832Z",
     "start_time": "2024-11-11T11:42:40.477590Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "\n",
    "def add_noise_to_data(data, noise_level=0.1):\n",
    "    \"\"\"\n",
    "    Ajoute un bruit gaussien aux données EEG.\n",
    "    Arguments :\n",
    "    - data : ndarray, les données EEG (n_epochs, n_channels, n_times).\n",
    "    - noise_level : float, intensité du bruit (fraction de l'amplitude max des données).\n",
    "    Retourne :\n",
    "    - data_noisy : ndarray, les données avec du bruit ajouté.\n",
    "    \"\"\"\n",
    "    noise = np.random.normal(0, noise_level * np.max(data), size=data.shape)\n",
    "    return data + noise\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87b5aced-834f-4cae-a7b9-dc948079391d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_and_save_noisy_data(base_dir, output_dir, noise_level=0.1):\n",
    "    \"\"\"\n",
    "    Parcourt les fichiers .fif, ajoute du bruit et sauvegarde les fichiers bruités.\n",
    "    Arguments :\n",
    "    - base_dir : str, répertoire contenant les fichiers originaux.\n",
    "    - output_dir : str, répertoire où sauvegarder les fichiers bruités.\n",
    "    - noise_level : float, intensité du bruit.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\"-epo.fif\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(root, base_dir)\n",
    "                output_path = os.path.join(output_dir, relative_path)\n",
    "                \n",
    "                # Créer les répertoires de sortie si nécessaires\n",
    "                os.makedirs(output_path, exist_ok=True)\n",
    "                \n",
    "                # Charger les données .fif\n",
    "                epochs = mne.read_epochs(file_path, preload=True)\n",
    "                data = epochs.get_data()  # (n_epochs, n_channels, n_times)\n",
    "\n",
    "                # Ajouter du bruit\n",
    "                data_noisy = add_noise_to_data(data, noise_level)\n",
    "\n",
    "                # Remplacer les données d'origine par les données bruitées\n",
    "                epochs._data = data_noisy\n",
    "\n",
    "                # Sauvegarder les données bruitées\n",
    "                noisy_file_path = os.path.join(output_path, file)\n",
    "                epochs.save(noisy_file_path, overwrite=True)\n",
    "                print(f\"Fichier bruité sauvegardé : {noisy_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d9427a6-fb55-4f51-9bef-92dc2005722d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/166600089/mathis-add-noises/processed\n",
      "/project/166600089/mathis-add-noises/noisy\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "base_dir = os.path.join(os.getcwd(), \"processed\")  # Répertoire de base contenant les fichiers originaux\n",
    "output_dir = os.path.join(os.getcwd(), \"noisy\") # Répertoire de sortie pour les fichiers bruités\n",
    "\n",
    "print(base_dir)\n",
    "print(output_dir)\n",
    "noise_level = 0.3  # Intensité du bruit\n",
    "\n",
    "process_and_save_noisy_data(base_dir, output_dir, noise_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c37280",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf7aa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_model = config['project_config']['running_model']\n",
    "\n",
    "subjects_id = config['data']['subjects']\n",
    "\n",
    "bids_root = config['data']['path']\n",
    "\n",
    "\n",
    "test_losses = []\n",
    "accuracies = []\n",
    "recalls = []\n",
    "precisions = []\n",
    "f1_scores = []\n",
    "auc_rocs = []\n",
    "\n",
    "if not subjects_id:\n",
    "        subjects_id = [\n",
    "            d for d in os.listdir(bids_root)\n",
    "            if os.path.isdir(os.path.join(bids_root, d)) and d.startswith(\"sub-\")\n",
    "        ]\n",
    "        print(f\"Aucun ID de sujet spécifié. Tous les sujets détectés : {subjects_id}\")\n",
    "\n",
    "for subject in subjects_id:\n",
    "\n",
    "    if running_model == \"GGN\":\n",
    "        train_loader, val_loader, test_loader = dataset.create_dataloader([subject], config)\n",
    "\n",
    "        model = GGN.GGN(**config['models']['GGN']['parameters'], device=device)\n",
    "        \n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        num_epochs = 50\n",
    "\n",
    "        # Train and validate the model\n",
    "        train.train(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\n",
    "\n",
    "        # Test the model\n",
    "        avg_test_loss, accuracy, recall, precision, f1, auc_roc = train.test(model, test_loader, criterion, device)\n",
    "        test_losses.append(avg_test_loss)\n",
    "        accuracies.append(accuracy)\n",
    "        recalls.append(recall)\n",
    "        precisions.append(precision)\n",
    "        f1_scores.append(f1)\n",
    "        auc_rocs.append(auc_roc)\n",
    "        \n",
    "        model.explain_temporal_cnn(test_loader, device)\n",
    "        \n",
    "\n",
    "    elif running_model == \"SVM\":\n",
    "        train_loader, val_loader, test_loader = dataset.create_dataloader([subject], config)\n",
    "\n",
    "        # Convert data loaders to numpy arrays\n",
    "        X_train, y_train = preprocess.dataloader_without_topology_to_numpy(train_loader)\n",
    "        X_val, y_val = preprocess.dataloader_without_topology_to_numpy(val_loader)\n",
    "        X_test, y_test = preprocess.dataloader_without_topology_to_numpy(test_loader)\n",
    "        \n",
    "        print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "        print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "        print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "        # Reshape data\n",
    "        X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "        X_val = X_val.reshape(X_val.shape[0], -1)\n",
    "        X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "        # Train the SVM\n",
    "        svm_params = config['models']['SVM']['parameters']\n",
    "        svm = SVC(**svm_params)\n",
    "        svm.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        y_val_pred = svm.predict(X_val)\n",
    "        val_acc = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "        # Test the model\n",
    "        y_test_pred = svm.predict(X_test)\n",
    "        test_acc = accuracy_score(y_test, y_test_pred)\n",
    "        accuracies.append(test_acc)\n",
    "\n",
    "# Calculate mean test accuracy across all subjects\n",
    "mean_test_loss = np.mean(test_losses)\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "mean_recall = np.mean(recalls)\n",
    "mean_precision = np.mean(precisions)\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "mean_auc_roc = np.mean(auc_rocs)\n",
    "\n",
    "print(f\"Mean Test Loss across all subjects: {mean_test_loss:.4f}\")\n",
    "print(f\"Mean Test Accuracy across all subjects: {mean_accuracy:.2f}%\")\n",
    "print(f\"Mean Recall (Sensitivity) across all subjects: {mean_recall:.2f}\")\n",
    "print(f\"Mean Precision across all subjects: {mean_precision:.2f}\")\n",
    "print(f\"Mean F1 Score across all subjects: {mean_f1:.2f}\")\n",
    "print(f\"Mean AUC-ROC across all subjects: {mean_auc_roc:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
